{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import glob\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from datetime import date\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "today = date.today()\n",
    "api = wandb.Api()\n",
    "\n",
    "# # Find all csv files in the current directory\n",
    "csv_files = glob.glob(\"/home/lev/projects/TopoBenchmarkX/big_csv/*.csv\")\n",
    "# # Collect all the names of the csv files without the extension\n",
    "csv_names = [csv_file[:-4] for csv_file in csv_files]\n",
    "project_name = \"TopoBenchmarkX_Hypergraph\"  \n",
    "user = \"telyatnikov_sap\"\n",
    "\n",
    "if project_name not in csv_names:\n",
    "    runs = api.runs(f\"{user}/{project_name}\")\n",
    "\n",
    "    summary_list, config_list, name_list = [], [], []\n",
    "    for run in runs:\n",
    "        # .summary contains the output keys/values for metrics like accuracy.\n",
    "        #  We call ._json_dict to omit large files\n",
    "        summary_list.append(run.summary._json_dict)\n",
    "\n",
    "        # .config contains the hyperparameters.\n",
    "        #  We remove special values that start with _.\n",
    "        config_list.append(\n",
    "            {k: v for k, v in run.config.items() if not k.startswith(\"_\")}\n",
    "        )\n",
    "\n",
    "        # .name is the human-readable name of the run.\n",
    "        name_list.append(run.name)\n",
    "\n",
    "    runs_df = pd.DataFrame(\n",
    "        {\"summary\": summary_list, \"config\": config_list, \"name\": name_list}\n",
    "    )\n",
    "\n",
    "    runs_df.to_csv(f\"{project_name}.csv\")\n",
    "else:\n",
    "    runs_df = pd.read_csv(f\"{project_name}.csv\", index_col=0)\n",
    "\n",
    "    for row in runs_df.iloc:\n",
    "        row[\"summary\"] = ast.literal_eval(row[\"summary\"])\n",
    "        row[\"config\"] = ast.literal_eval(row[\"config\"])\n",
    "\n",
    "\n",
    "for row in runs_df.iloc:\n",
    "    row[\"summary\"].update(row[\"config\"])\n",
    "\n",
    "lst = [i[\"summary\"] for i in runs_df.iloc]\n",
    "df = pd.DataFrame.from_dict(lst)\n",
    "\n",
    "df_init = df.copy()\n",
    "\n",
    "# Get average epoch run time\n",
    "df[\"epoch_run_time\"] = df[\"_runtime\"] / df[\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_column(df, column_to_normalize):\n",
    "    # Use json_normalize to flatten the nested dictionaries into separate columns\n",
    "    flattened_df = pd.json_normalize(df[column_to_normalize])\n",
    "    # Rename columns to include 'nested_column' prefix\n",
    "    flattened_df.columns = [\n",
    "        f\"{column_to_normalize}.{col}\" for col in flattened_df.columns\n",
    "    ]\n",
    "    # Concatenate the flattened DataFrame with the original DataFrame\n",
    "    result_df = pd.concat([df, flattened_df], axis=1)\n",
    "    # Get new columns names\n",
    "    new_columns = flattened_df.columns\n",
    "    # Drop the original nested column if needed\n",
    "    result_df.drop(column_to_normalize, axis=1, inplace=True)\n",
    "    return result_df, new_columns\n",
    "\n",
    "\n",
    "# Config columns to normalize\n",
    "columns_to_normalize = [\"model\", \"dataset\", \"callbacks\", \"paths\"]\n",
    "\n",
    "# Keep track of config columns added\n",
    "config_columns = []\n",
    "for column in columns_to_normalize:\n",
    "    df, columns = normalize_column(df, column)\n",
    "    config_columns.extend(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select models that have finished the runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workout us_demographic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every rows where df['dataset.parameters.data_name'] == 'US-county-demos' extend the 'dataset.parameters.data_name' with dataset.parameters.task_variable \n",
    "# and set it to 'US-county-demos' + '-' + dataset.parameters.task_variable\n",
    "df.loc[df['dataset.parameters.data_name'] == 'US-county-demos', 'dataset.parameters.data_name'] = df.loc[df['dataset.parameters.data_name'] == 'US-county-demos', 'dataset.parameters.data_name'] + '-' + df.loc[df['dataset.parameters.data_name'] == 'US-county-demos', 'dataset.parameters.task_variable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_runtime', 'test/loss', 'train/loss', 'train/auroc', 'train/precision']\n",
      "['_step', 'lr-Adam', 'val/loss', 'val/auroc', '_timestamp']\n",
      "['test/auroc', 'trainer/global_step', 'epoch', '_wandb', 'val/recall']\n",
      "['train/recall', 'test/precision', 'train/accuracy', 'test/recall', 'val/accuracy']\n",
      "['test/accuracy', 'val/precision', 'seed', 'tags', 'extras']\n",
      "['trainer', 'ckpt_path', 'task_name', 'model/params/total', 'model/params/trainable']\n",
      "['model/params/non_trainable', 'test/mae', 'test/mse', 'train/mae', 'val/mse']\n",
      "['train/mse', 'val/mae', 'epoch_run_time', 'model.compile', 'model._target_']\n",
      "['model.model_name', 'model.model_domain', 'model.loss.task', 'model.loss._target_', 'model.loss.loss_type']\n",
      "['model.readout._target_', 'model.readout.hidden_dim', 'model.readout.readout_name', 'model.readout.num_cell_dimensions', 'model.backbone.beta']\n",
      "['model.backbone.alpha', 'model.backbone._target_', 'model.backbone.n_layers', 'model.backbone.input_drop', 'model.backbone.layer_drop']\n",
      "['model.backbone.in_channels', 'model.backbone.hidden_channels', 'model.optimizer.lr', 'model.optimizer._target_', 'model.optimizer._partial_']\n",
      "['model.optimizer.weight_decay', 'model.scheduler.gamma', 'model.scheduler._target_', 'model.scheduler._partial_', 'model.scheduler.step_size']\n",
      "['model.head_model._target_', 'model.head_model.task_level', 'model.head_model.in_channels', 'model.head_model.out_channels', 'model.head_model.pooling_type']\n",
      "['model.head_model.head_model_name', 'model.feature_encoder._target_', 'model.feature_encoder.in_channels', 'model.feature_encoder.encoder_name', 'model.feature_encoder.out_channels']\n",
      "['model.feature_encoder.proj_dropout', 'model.backbone_wrapper._target_', 'model.backbone_wrapper._partial_', 'model.backbone_wrapper.out_channels', 'model.backbone_wrapper.wrapper_name']\n",
      "['model.backbone_wrapper.num_cell_dimensions', 'model.backbone.dropout', 'model.backbone.aggregate', 'model.backbone.activation', 'model.backbone.edconv_type']\n",
      "['model.backbone.num_features', 'model.backbone.input_dropout', 'model.backbone.All_num_layers', 'model.backbone.MLP_num_layers', 'model.backbone.heads']\n",
      "['model.backbone.mlp_dropout', 'model.backbone.mlp_num_layers', 'dataset._target_', 'dataset.parameters.k', 'dataset.parameters.task']\n",
      "['dataset.parameters.data_dir', 'dataset.parameters.data_name', 'dataset.parameters.data_seed', 'dataset.parameters.data_type', 'dataset.parameters.loss_type']\n",
      "['dataset.parameters.batch_size', 'dataset.parameters.pin_memory', 'dataset.parameters.split_type', 'dataset.parameters.task_level', 'dataset.parameters.train_prop']\n",
      "['dataset.parameters.data_domain', 'dataset.parameters.num_classes', 'dataset.parameters.num_workers', 'dataset.parameters.num_features', 'dataset.parameters.data_split_dir']\n",
      "['dataset.parameters.monitor_metric', 'dataset.parameters.max_node_degree', 'dataset.transforms.data_manipulations.std', 'dataset.transforms.data_manipulations.mean', 'dataset.transforms.data_manipulations._target_']\n",
      "['dataset.transforms.data_manipulations.num_features', 'dataset.transforms.data_manipulations.transform_name', 'dataset.transforms.data_manipulations.transform_type', 'dataset.transforms.graph2hypergraph_lifting.k_value', 'dataset.transforms.graph2hypergraph_lifting._target_']\n",
      "['dataset.transforms.graph2hypergraph_lifting.transform_name', 'dataset.transforms.graph2hypergraph_lifting.transform_type', 'dataset.parameters.force_reload', 'dataset.parameters.max_x_1_degree', 'dataset.transforms.data_manipulations.selected_fields']\n",
      "['dataset.transforms.one_hot_node_degree_features._target_', 'dataset.transforms.one_hot_node_degree_features.max_degrees', 'dataset.transforms.one_hot_node_degree_features.degrees_fields', 'dataset.transforms.one_hot_node_degree_features.transform_name', 'dataset.transforms.one_hot_node_degree_features.transform_type']\n",
      "['dataset.transforms.one_hot_node_degree_features.features_fields', 'dataset.parameters.max_dim_if_lifted', 'dataset.parameters.preserve_edge_attr_if_lifted', 'dataset.parameters.year', 'dataset.parameters.task_variable']\n",
      "['callbacks.model_summary._target_', 'callbacks.model_summary.max_depth', 'callbacks.early_stopping.mode', 'callbacks.early_stopping.strict', 'callbacks.early_stopping.monitor']\n",
      "['callbacks.early_stopping.verbose', 'callbacks.early_stopping._target_', 'callbacks.early_stopping.patience', 'callbacks.early_stopping.min_delta', 'callbacks.early_stopping.check_finite']\n",
      "['callbacks.early_stopping.stopping_threshold', 'callbacks.early_stopping.divergence_threshold', 'callbacks.early_stopping.check_on_train_epoch_end', 'callbacks.model_checkpoint.mode', 'callbacks.model_checkpoint.dirpath']\n",
      "['callbacks.model_checkpoint.monitor', 'callbacks.model_checkpoint.verbose', 'callbacks.model_checkpoint._target_', 'callbacks.model_checkpoint.filename', 'callbacks.model_checkpoint.save_last']\n",
      "['callbacks.model_checkpoint.save_top_k', 'callbacks.model_checkpoint.every_n_epochs', 'callbacks.model_checkpoint.save_weights_only', 'callbacks.model_checkpoint.every_n_train_steps', 'callbacks.model_checkpoint.train_time_interval']\n",
      "['callbacks.model_checkpoint.auto_insert_metric_name', 'callbacks.model_checkpoint.save_on_train_epoch_end', 'callbacks.rich_progress_bar._target_', 'callbacks.learning_rate_monitor._target_', 'callbacks.learning_rate_monitor.logging_interval']\n",
      "['paths.log_dir', 'paths.data_dir', 'paths.root_dir', 'paths.work_dir', 'paths.output_dir']\n"
     ]
    }
   ],
   "source": [
    "# Print all columns 10 per line\n",
    "for i in range(0, len(df.columns), 5):\n",
    "    print(list(df.columns[i:i + 5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See unique datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['REDDIT-BINARY' 'minesweeper' 'tolokers' 'amazon_ratings' 'roman_empire'\n",
      " 'IMDB-MULTI' 'IMDB-BINARY' 'NCI109' 'NCI1' 'PROTEINS' 'MUTAG' 'ZINC'\n",
      " 'PubMed' 'citeseer' 'Cora' 'US-county-demos-UnemploymentRate'\n",
      " 'US-county-demos-BachelorRate' 'US-county-demos-DeathRate'\n",
      " 'US-county-demos-BirthRate' 'US-county-demos-MigraRate'\n",
      " 'US-county-demos-MedianIncome' 'US-county-demos-Election']\n",
      "Num unique datasets: 22\n"
     ]
    }
   ],
   "source": [
    "print(df['dataset.parameters.data_name'].unique())\n",
    "print(\"Num unique datasets:\", len(df['dataset.parameters.data_name'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See unique models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unignn2' 'edgnn' 'allsettransformer']\n"
     ]
    }
   ],
   "source": [
    "print(df['model.model_name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL: unignn2\n",
      "Dataset: minesweeper  [1]\n",
      "Dataset: questions  []\n",
      "Dataset: tolokers  [1]\n",
      "Dataset: amazon_ratings  [1]\n",
      "Dataset: roman_empire  [1]\n",
      "MODEL: allsettransformer\n",
      "Dataset: minesweeper  [1]\n",
      "Dataset: questions  []\n",
      "Dataset: tolokers  [1]\n",
      "Dataset: amazon_ratings  [1]\n",
      "Dataset: roman_empire  [1]\n",
      "MODEL: edgnn\n",
      "Dataset: minesweeper  [128 256]\n",
      "Dataset: questions  []\n",
      "Dataset: tolokers  [128 256]\n",
      "Dataset: amazon_ratings  [128 256]\n",
      "Dataset: roman_empire  [128 256]\n"
     ]
    }
   ],
   "source": [
    "datasets = ['minesweeper', 'questions', 'tolokers', 'amazon_ratings', 'roman_empire']\n",
    "models = ['unignn2', 'allsettransformer', 'edgnn']\n",
    "# For the following models and datasets I mistook the batch size, it should be 1, instead of 256 or 128\n",
    "# Keep the run where batch size is 128 and then change the batch size to 1\n",
    "for model in models:\n",
    "    print(\"MODEL:\", model)\n",
    "    for dataset in datasets:\n",
    "        # Change the batch size to 1 when it is 128\n",
    "        print(f\"Dataset: {dataset} \", df.loc[(df['model.model_name'] == model) & (df['dataset.parameters.data_name'] == dataset), 'dataset.parameters.batch_size'].unique())\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve batch problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['minesweeper', 'questions', 'tolokers', 'amazon_ratings', 'roman_empire']\n",
    "models = ['edgnn']\n",
    "# For the following models and datasets I mistook the batch size, it should be 1, instead of 256 or 128\n",
    "# Keep the run where batch size is 128 and then change the batch size to 1\n",
    "for model in models:\n",
    "    for dataset in datasets:\n",
    "        # Change the batch size to 1 when it is 128\n",
    "        df.loc[(df['model.model_name'] == model) & (df['dataset.parameters.data_name'] == dataset) & (df['dataset.parameters.batch_size'] == 128), 'dataset.parameters.batch_size'] = 1\n",
    "        # Drop runs where batch size is 256\n",
    "        df.drop(df[(df['model.model_name'] == model) & (df['dataset.parameters.data_name'] == dataset) & (df['dataset.parameters.batch_size'] == 256)].index, inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.loc[(df['model.model_name'] == 'edgnn') & (df['dataset.parameters.data_name'] == 'minesweeper') & (df['dataset.parameters.batch_size'] == 128), 'dataset.parameters.batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve issue with projection dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5  0.25]\n"
     ]
    }
   ],
   "source": [
    "print(df['model.feature_encoder.proj_dropout'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep rows where model.feature_encoder.proj_dropout is [0.5  0.25]\n",
    "df = df[df['model.feature_encoder.proj_dropout'].isin([0.5, 0.25])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sweeped parameters: \n",
    "# sweeped_columns = [\n",
    "#     'model.optimizer.lr', \n",
    "#     'model.feature_encoder.out_channels',\n",
    "#     'model.backbone.n_layers',\n",
    "#     'model.backbone.All_num_layers',\n",
    "#     'model.feature_encoder.proj_dropout',\n",
    "#     'dataset.parameters.batch_size',\n",
    "#     'dataset.parameters.data_seed',\n",
    "#     'seed',\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "# # For each model and dataset go over all the sweeped parameters and print the unique values\n",
    "# for model in df['model.model_name'].unique():\n",
    "#     print(f\"Model: {model}\")\n",
    "#     for dataset in df['dataset.parameters.data_name'].unique():\n",
    "#         print(f\"Dataset: {dataset}\")\n",
    "#         for column in sweeped_columns:\n",
    "#             print(f\"Column: {column}\")\n",
    "#             print(df.loc[(df['model.model_name'] == model) & (df['dataset.parameters.data_name'] == dataset), column].unique())\n",
    "        \n",
    "#         print('---------------NEW DATASET------------------')\n",
    "#     print('---------------NEW MODEL------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best results for each model and dataset\n",
    "# 1. Keep the columns that are necessary for the comparison\n",
    "sweeped_columns = [\n",
    "    'model.optimizer.lr', \n",
    "    'model.feature_encoder.out_channels',\n",
    "    'model.backbone.n_layers',\n",
    "    'model.backbone.All_num_layers',\n",
    "    'model.feature_encoder.proj_dropout',\n",
    "    'dataset.parameters.batch_size',\n",
    "    'model/params/total',\n",
    "    # 'dataset.parameters.data_seed',\n",
    "    # 'seed',\n",
    "]\n",
    "run_columns = ['dataset.parameters.data_seed','seed']\n",
    "\n",
    "# Dataset and model columns\n",
    "dataset_model_columns = ['model.model_name', 'dataset.parameters.data_name']\n",
    "\n",
    "# Performance columns\n",
    "performance_columns = [\n",
    "    'val/loss', 'test/loss',\n",
    "    'val/mae', 'test/mae',\n",
    "    'val/mse', 'test/mse',\n",
    "    'val/accuracy', 'test/accuracy',\n",
    "    'val/auroc','test/auroc',\n",
    "    'val/recall', 'test/recall',\n",
    "    'val/precision', 'test/precision',\n",
    "    ]\n",
    "keep_columns = dataset_model_columns + sweeped_columns + performance_columns + run_columns\n",
    "df = df[keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_classification = [\n",
    "    'val/accuracy', 'test/accuracy',\n",
    "    'val/auroc','test/auroc',\n",
    "    'val/recall', 'test/recall',\n",
    "    'val/precision', 'test/precision',\n",
    "    ]\n",
    "performance_regression = [\n",
    "    'val/mae', 'test/mae',\n",
    "    'val/mse', 'test/mse',\n",
    "    ]\n",
    "# Define a dict of dicts for each dataset the corresponding optimization metrics\n",
    "optimization_metrics = {\n",
    "    'IMDB-MULTI': {'optim_metric': 'val/accuracy', 'eval_metric': 'test/accuracy', 'direction': 'max', 'performance_columns': performance_classification},\n",
    "    'IMDB-BINARY': {'optim_metric': 'val/accuracy', 'eval_metric': 'test/accuracy', 'direction': 'max', 'performance_columns': performance_classification},\n",
    "    'REDDIT-BINARY': {'optim_metric': 'val/accuracy', 'eval_metric': 'test/accuracy', 'direction': 'max', 'performance_columns': performance_classification},\n",
    "    'NCI109': {'optim_metric': 'val/accuracy', 'eval_metric': 'test/accuracy', 'direction': 'max', 'performance_columns': performance_classification},\n",
    "    'NCI1': {'optim_metric': 'val/accuracy', 'eval_metric': 'test/accuracy', 'direction': 'max', 'performance_columns': performance_classification},\n",
    "    'PROTEINS': {'optim_metric': 'val/accuracy', 'eval_metric': 'test/accuracy', 'direction': 'max', 'performance_columns': performance_classification},\n",
    "    'MUTAG': {'optim_metric': 'val/accuracy', 'eval_metric': 'test/accuracy', 'direction': 'max', 'performance_columns': performance_classification},\n",
    "    'Cora': {'optim_metric': 'val/accuracy', 'eval_metric': 'test/accuracy', 'direction': 'max', 'performance_columns': performance_classification},\n",
    "    'citeseer': {'optim_metric': 'val/accuracy', 'eval_metric': 'test/accuracy', 'direction': 'max', 'performance_columns': performance_classification},\n",
    "    'PubMed': {'optim_metric': 'val/accuracy', 'eval_metric': 'test/accuracy', 'direction': 'max', 'performance_columns': performance_classification},\n",
    "\n",
    "    'roman_empire': {'optim_metric': 'val/accuracy', 'eval_metric': 'test/accuracy', 'direction': 'max', 'performance_columns': performance_classification},\n",
    "    'amazon_ratings': {'optim_metric': 'val/accuracy', 'eval_metric': 'test/accuracy', 'direction': 'max', 'performance_columns': performance_classification},\n",
    "    \n",
    "    'tolokers': {'optim_metric': 'val/auroc', 'eval_metric': 'test/auroc', 'direction': 'max', 'performance_columns': performance_classification},\n",
    "    'questions': {'optim_metric': 'val/auroc', 'eval_metric': 'test/auroc', 'direction': 'max', 'performance_columns': performance_classification},\n",
    "    'minesweeper': {'optim_metric': 'val/auroc', 'eval_metric': 'test/auroc', 'direction': 'max', 'performance_columns': performance_classification},\n",
    "\n",
    "    'ZINC': {'optim_metric': 'val/mae', 'eval_metric': 'test/mae', 'direction': 'min', 'performance_columns': performance_regression},\n",
    "    \n",
    "    'US-county-demos-UnemploymentRate': {'optim_metric': 'val/mse', 'eval_metric': 'test/mse', 'direction': 'min', 'performance_columns': performance_regression},\n",
    "    'US-county-demos-BachelorRate': {'optim_metric': 'val/mse', 'eval_metric': 'test/mse', 'direction': 'min', 'performance_columns': performance_regression},\n",
    "    'US-county-demos-DeathRate': {'optim_metric': 'val/mse', 'eval_metric': 'test/mse', 'direction': 'min', 'performance_columns': performance_regression},\n",
    "    'US-county-demos-BirthRate': {'optim_metric': 'val/mse', 'eval_metric': 'test/mse', 'direction': 'min', 'performance_columns': performance_regression},\n",
    "    'US-county-demos-MigraRate': {'optim_metric': 'val/mse', 'eval_metric': 'test/mse', 'direction': 'min', 'performance_columns': performance_regression},\n",
    "    'US-county-demos-MedianIncome': {'optim_metric': 'val/mse', 'eval_metric': 'test/mse', 'direction': 'min', 'performance_columns': performance_regression},\n",
    "    'US-county-demos-Election': {'optim_metric': 'val/mse', 'eval_metric': 'test/mse', 'direction': 'min', 'performance_columns': performance_regression},\n",
    "\n",
    "} \n",
    "\n",
    "len(optimization_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique datasets\n",
    "datasets = list(df['dataset.parameters.data_name'].unique())\n",
    "# Get unique models\n",
    "models = list(df['model.model_name'].unique())\n",
    "\n",
    "best_results = defaultdict(dict)\n",
    "best_results_all_metrics = defaultdict(dict)\n",
    "best_runs = defaultdict(dict)\n",
    "collect_bast_parameters = defaultdict(dict)\n",
    "# Got over each dataset and model and find the best result\n",
    "for dataset in datasets:\n",
    "    for model in models:\n",
    "        # Get the subset of the DataFrame for the current dataset and model\n",
    "        subset = df[\n",
    "            (df['dataset.parameters.data_name'] == dataset)\n",
    "            & (df['model.model_name'] == model)\n",
    "        ]\n",
    "\n",
    "        optim_metric = optimization_metrics[dataset]['optim_metric']\n",
    "        eval_metric = optimization_metrics[dataset]['eval_metric']\n",
    "        direction = optimization_metrics[dataset]['direction']\n",
    "        \n",
    "        # Keep metrics that matters for dataset\n",
    "        performance_columns = optimization_metrics[dataset]['performance_columns']\n",
    "        subset = subset[dataset_model_columns + sweeped_columns + performance_columns + run_columns]\n",
    "\n",
    "        aggregated = subset.groupby(sweeped_columns, dropna=False).agg(\n",
    "            {col: [\"mean\", \"std\"] for col in performance_columns}\n",
    "        )\n",
    "\n",
    "         # Go from MultiIndex to Index\n",
    "        aggregated = aggregated.reset_index()\n",
    "        aggregated = aggregated.sort_values(\n",
    "                by=(optim_metric, \"mean\"), ascending=(direction == 'min')\n",
    "            )\n",
    "        \n",
    "        # Git percent in case of classification\n",
    "        if 'test/accuracy' in performance_columns:\n",
    "            # Go over all the performance columns and multiply by 100\n",
    "            for col in performance_columns:\n",
    "                aggregated[(col, \"mean\")] *= 100\n",
    "                aggregated[(col, \"std\")] *= 100\n",
    "            \n",
    "            # Round performance columns values up to 2 decimal points\n",
    "            for col in performance_columns:\n",
    "                aggregated[(col, \"mean\")] = aggregated[(col, \"mean\")].round(2)\n",
    "                aggregated[(col, \"std\")] = aggregated[(col, \"std\")].round(2)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            # Round all values up to 4 decimal points\n",
    "            # Round performance columns values up to 4 decimal points\n",
    "            for col in performance_columns:\n",
    "                aggregated[(col, \"mean\")] = aggregated[(col, \"mean\")].round(4)\n",
    "                aggregated[(col, \"std\")] = aggregated[(col, \"std\")].round(4)\n",
    "        \n",
    "            \n",
    "        \n",
    "        # Get the best result\n",
    "        final_best = aggregated.head(1)\n",
    "        if final_best[(eval_metric, \"mean\")].any(): \n",
    "            best_results[dataset][model] = {\n",
    "                \"mean\": final_best[(eval_metric, \"mean\")].values[0],\n",
    "                \"std\": final_best[(eval_metric, \"std\")].values[0],\n",
    "            }\n",
    "\n",
    "            # Extract best runs: \n",
    "            best_params = {}\n",
    "            for col in sweeped_columns:\n",
    "                best_params[col] = final_best[(col, '')].item()\n",
    "            \n",
    "            collect_bast_parameters[dataset][model] = best_params\n",
    "            #hp_runs[dataset][model] = subset.copy()\n",
    "            \n",
    "            # Start with the entire DataFrame\n",
    "            filtered_subset = subset.copy()\n",
    "\n",
    "            # Iterate over each key-value pair in the best parameters dictionary and filter the DataFrame\n",
    "            for param, value in best_params.items():\n",
    "                filtered_subset = filtered_subset[filtered_subset[param] == value]\n",
    "            best_runs[dataset][model] = filtered_subset\n",
    "        \n",
    "        else: \n",
    "            best_results[dataset][model] = {\n",
    "                \"mean\": np.nan,\n",
    "                \"std\": np.nan,\n",
    "            }\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save obtained best results and best runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert nested dictionary to DataFrame\n",
    "nested_dict = dict(best_results)\n",
    "result_dict = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        (i, j): nested_dict[i][j]\n",
    "        for i in nested_dict\n",
    "        for j in nested_dict[i].keys()\n",
    "    },\n",
    "    orient=\"index\",\n",
    ")\n",
    "\n",
    "result_dict[\"performance\"] = result_dict.apply(\n",
    "    lambda x: f\"{x['mean']} ± {x['std']}\", axis=1\n",
    ")\n",
    "result_dict = result_dict.drop([\"mean\", \"std\"], axis=1)\n",
    "\n",
    "# Reset multiindex\n",
    "result_dict = result_dict.reset_index()\n",
    "# rename columns\n",
    "result_dict.columns = [\"Dataset\", \"Model\", \"Performance\"]\n",
    "\n",
    "result_dict = result_dict.pivot_table(\n",
    "    index=\"Model\", columns=\"Dataset\", values=\"Performance\", aggfunc=\"first\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the number of allowed rows to display\n",
    "pd.set_option(\"display.max_rows\", 1000)\n",
    "pd.set_option(\"display.max_columns\", 1000)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "result_dict.to_csv(f\"best_results_hypergraph.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Dataset</th>\n",
       "      <th>Cora</th>\n",
       "      <th>IMDB-BINARY</th>\n",
       "      <th>IMDB-MULTI</th>\n",
       "      <th>MUTAG</th>\n",
       "      <th>NCI1</th>\n",
       "      <th>NCI109</th>\n",
       "      <th>PROTEINS</th>\n",
       "      <th>PubMed</th>\n",
       "      <th>REDDIT-BINARY</th>\n",
       "      <th>US-county-demos-BachelorRate</th>\n",
       "      <th>US-county-demos-BirthRate</th>\n",
       "      <th>US-county-demos-DeathRate</th>\n",
       "      <th>US-county-demos-Election</th>\n",
       "      <th>US-county-demos-MedianIncome</th>\n",
       "      <th>US-county-demos-MigraRate</th>\n",
       "      <th>US-county-demos-UnemploymentRate</th>\n",
       "      <th>ZINC</th>\n",
       "      <th>amazon_ratings</th>\n",
       "      <th>citeseer</th>\n",
       "      <th>minesweeper</th>\n",
       "      <th>roman_empire</th>\n",
       "      <th>tolokers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>allsettransformer</th>\n",
       "      <td>88.92 ± 0.44</td>\n",
       "      <td>70.32 ± 3.27</td>\n",
       "      <td>50.51 ± 2.92</td>\n",
       "      <td>71.06 ± 6.49</td>\n",
       "      <td>75.18 ± 1.24</td>\n",
       "      <td>73.75 ± 1.09</td>\n",
       "      <td>76.63 ± 1.74</td>\n",
       "      <td>89.62 ± 0.25</td>\n",
       "      <td>74.84 ± 2.68</td>\n",
       "      <td>0.2996 ± 0.0276</td>\n",
       "      <td>0.7111 ± 0.0823</td>\n",
       "      <td>0.4938 ± 0.0456</td>\n",
       "      <td>0.2897 ± 0.0136</td>\n",
       "      <td>0.2066 ± 0.0216</td>\n",
       "      <td>0.7775 ± 0.123</td>\n",
       "      <td>0.2199 ± 0.022</td>\n",
       "      <td>0.59 ± 0.0233</td>\n",
       "      <td>50.5 ± 0.27</td>\n",
       "      <td>73.85 ± 2.21</td>\n",
       "      <td>81.14 ± 0.05</td>\n",
       "      <td>79.5 ± 0.13</td>\n",
       "      <td>83.26 ± 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edgnn</th>\n",
       "      <td>87.06 ± 1.09</td>\n",
       "      <td>69.12 ± 2.92</td>\n",
       "      <td>49.17 ± 4.35</td>\n",
       "      <td>80.0 ± 4.9</td>\n",
       "      <td>73.97 ± 0.82</td>\n",
       "      <td>74.93 ± 2.5</td>\n",
       "      <td>73.91 ± 4.39</td>\n",
       "      <td>89.04 ± 0.51</td>\n",
       "      <td>83.24 ± 1.45</td>\n",
       "      <td>0.2934 ± 0.0242</td>\n",
       "      <td>0.7036 ± 0.0743</td>\n",
       "      <td>0.5192 ± 0.0466</td>\n",
       "      <td>0.3418 ± 0.0246</td>\n",
       "      <td>0.225 ± 0.0236</td>\n",
       "      <td>0.7988 ± 0.1225</td>\n",
       "      <td>0.2576 ± 0.0281</td>\n",
       "      <td>0.5109 ± 0.011</td>\n",
       "      <td>48.18 ± 0.09</td>\n",
       "      <td>74.93 ± 1.39</td>\n",
       "      <td>84.52 ± 0.05</td>\n",
       "      <td>81.01 ± 0.24</td>\n",
       "      <td>77.53 ± 0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unignn2</th>\n",
       "      <td>86.97 ± 0.88</td>\n",
       "      <td>71.04 ± 1.31</td>\n",
       "      <td>49.76 ± 3.55</td>\n",
       "      <td>80.43 ± 4.09</td>\n",
       "      <td>73.02 ± 0.92</td>\n",
       "      <td>70.76 ± 1.11</td>\n",
       "      <td>75.2 ± 2.96</td>\n",
       "      <td>89.34 ± 0.45</td>\n",
       "      <td>75.56 ± 3.19</td>\n",
       "      <td>0.311 ± 0.0229</td>\n",
       "      <td>0.7257 ± 0.0952</td>\n",
       "      <td>0.511 ± 0.0454</td>\n",
       "      <td>0.367 ± 0.0204</td>\n",
       "      <td>0.2342 ± 0.0218</td>\n",
       "      <td>0.7923 ± 0.1162</td>\n",
       "      <td>0.2833 ± 0.0207</td>\n",
       "      <td>0.5991 ± 0.0056</td>\n",
       "      <td>49.06 ± 0.08</td>\n",
       "      <td>74.72 ± 1.08</td>\n",
       "      <td>78.02 ± 0.0</td>\n",
       "      <td>77.06 ± 0.2</td>\n",
       "      <td>77.35 ± 0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Dataset                    Cora   IMDB-BINARY    IMDB-MULTI         MUTAG          NCI1        NCI109      PROTEINS        PubMed REDDIT-BINARY US-county-demos-BachelorRate US-county-demos-BirthRate US-county-demos-DeathRate US-county-demos-Election US-county-demos-MedianIncome US-county-demos-MigraRate US-county-demos-UnemploymentRate             ZINC amazon_ratings      citeseer   minesweeper  roman_empire      tolokers\n",
       "Model                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "allsettransformer  88.92 ± 0.44  70.32 ± 3.27  50.51 ± 2.92  71.06 ± 6.49  75.18 ± 1.24  73.75 ± 1.09  76.63 ± 1.74  89.62 ± 0.25  74.84 ± 2.68              0.2996 ± 0.0276           0.7111 ± 0.0823           0.4938 ± 0.0456          0.2897 ± 0.0136              0.2066 ± 0.0216            0.7775 ± 0.123                   0.2199 ± 0.022    0.59 ± 0.0233    50.5 ± 0.27  73.85 ± 2.21  81.14 ± 0.05   79.5 ± 0.13   83.26 ± 0.1\n",
       "edgnn              87.06 ± 1.09  69.12 ± 2.92  49.17 ± 4.35    80.0 ± 4.9  73.97 ± 0.82   74.93 ± 2.5  73.91 ± 4.39  89.04 ± 0.51  83.24 ± 1.45              0.2934 ± 0.0242           0.7036 ± 0.0743           0.5192 ± 0.0466          0.3418 ± 0.0246               0.225 ± 0.0236           0.7988 ± 0.1225                  0.2576 ± 0.0281   0.5109 ± 0.011   48.18 ± 0.09  74.93 ± 1.39  84.52 ± 0.05  81.01 ± 0.24  77.53 ± 0.01\n",
       "unignn2            86.97 ± 0.88  71.04 ± 1.31  49.76 ± 3.55  80.43 ± 4.09  73.02 ± 0.92  70.76 ± 1.11   75.2 ± 2.96  89.34 ± 0.45  75.56 ± 3.19               0.311 ± 0.0229           0.7257 ± 0.0952            0.511 ± 0.0454           0.367 ± 0.0204              0.2342 ± 0.0218           0.7923 ± 0.1162                  0.2833 ± 0.0207  0.5991 ± 0.0056   49.06 ± 0.08  74.72 ± 1.08   78.02 ± 0.0   77.06 ± 0.2  77.35 ± 0.03"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"hypergraph\"\n",
    "# Convert nested dictionary to DataFrame\n",
    "nested_dict = dict(collect_bast_parameters)\n",
    "result_dict = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        (i, j): nested_dict[i][j]\n",
    "        for i in nested_dict\n",
    "        for j in nested_dict[i].keys()\n",
    "    },\n",
    "    orient=\"index\",\n",
    ")\n",
    "\n",
    "\n",
    "result_dict = result_dict.reset_index()\n",
    "\n",
    "rename_cols = {\n",
    "#'model.optimizer.lr': \"lr\",\n",
    "#'model.feature_encoder.out_channels':\"out_channels\",\n",
    "#'model.backbone.num_layers': 'num_layers',\n",
    "#'model.feature_encoder.proj_dropout': 'proj_dropout',\n",
    "#'dataset.parameters.batch_size': 'batch_size',\n",
    "'level_0': 'Dataset', 'level_1': 'Model', 'model/params/total': 'Num. parameters'}\n",
    "\n",
    "result_dict.rename(columns=rename_cols, inplace=True)\n",
    "result_dict['Num. parameters'] = (result_dict['Num. parameters'] /1000).round(2).apply(lambda x: f\"{x}K\")\n",
    "result_dict.to_csv(f\"best_parameters_{domain}.csv\")\n",
    "\n",
    "result_dict = result_dict.pivot_table(\n",
    "    index=\"Model\", columns=\"Dataset\", values='Num. parameters', aggfunc=\"first\"\n",
    ")\n",
    "result_dict.to_csv(f\"model_sizes_{domain}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Dataset</th>\n",
       "      <th>Cora</th>\n",
       "      <th>IMDB-BINARY</th>\n",
       "      <th>IMDB-MULTI</th>\n",
       "      <th>MUTAG</th>\n",
       "      <th>NCI1</th>\n",
       "      <th>NCI109</th>\n",
       "      <th>PROTEINS</th>\n",
       "      <th>PubMed</th>\n",
       "      <th>REDDIT-BINARY</th>\n",
       "      <th>US-county-demos-BachelorRate</th>\n",
       "      <th>US-county-demos-BirthRate</th>\n",
       "      <th>US-county-demos-DeathRate</th>\n",
       "      <th>US-county-demos-Election</th>\n",
       "      <th>US-county-demos-MedianIncome</th>\n",
       "      <th>US-county-demos-MigraRate</th>\n",
       "      <th>US-county-demos-UnemploymentRate</th>\n",
       "      <th>ZINC</th>\n",
       "      <th>amazon_ratings</th>\n",
       "      <th>citeseer</th>\n",
       "      <th>minesweeper</th>\n",
       "      <th>roman_empire</th>\n",
       "      <th>tolokers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>allsettransformer</th>\n",
       "      <td>60.26K</td>\n",
       "      <td>114.24K</td>\n",
       "      <td>111.3K</td>\n",
       "      <td>80.77K</td>\n",
       "      <td>57.47K</td>\n",
       "      <td>221.57K</td>\n",
       "      <td>14.34K</td>\n",
       "      <td>280.83K</td>\n",
       "      <td>106.18K</td>\n",
       "      <td>316.93K</td>\n",
       "      <td>30.21K</td>\n",
       "      <td>316.93K</td>\n",
       "      <td>217.34K</td>\n",
       "      <td>217.34K</td>\n",
       "      <td>80.64K</td>\n",
       "      <td>105.86K</td>\n",
       "      <td>106.82K</td>\n",
       "      <td>155.91K</td>\n",
       "      <td>132.87K</td>\n",
       "      <td>118.02K</td>\n",
       "      <td>257.17K</td>\n",
       "      <td>217.99K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edgnn</th>\n",
       "      <td>113.29K</td>\n",
       "      <td>9.86K</td>\n",
       "      <td>27.01K</td>\n",
       "      <td>5.73K</td>\n",
       "      <td>88.19K</td>\n",
       "      <td>88.32K</td>\n",
       "      <td>5.6K</td>\n",
       "      <td>147.59K</td>\n",
       "      <td>5.83K</td>\n",
       "      <td>84.1K</td>\n",
       "      <td>84.1K</td>\n",
       "      <td>84.1K</td>\n",
       "      <td>21.57K</td>\n",
       "      <td>21.57K</td>\n",
       "      <td>21.57K</td>\n",
       "      <td>21.57K</td>\n",
       "      <td>22.53K</td>\n",
       "      <td>122.24K</td>\n",
       "      <td>258.5K</td>\n",
       "      <td>21.7K</td>\n",
       "      <td>41.49K</td>\n",
       "      <td>21.89K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unignn2</th>\n",
       "      <td>109.06K</td>\n",
       "      <td>100.61K</td>\n",
       "      <td>8.32K</td>\n",
       "      <td>84.1K</td>\n",
       "      <td>104.32K</td>\n",
       "      <td>4.61K</td>\n",
       "      <td>21.31K</td>\n",
       "      <td>114.56K</td>\n",
       "      <td>68.1K</td>\n",
       "      <td>3.55K</td>\n",
       "      <td>13.25K</td>\n",
       "      <td>51.07K</td>\n",
       "      <td>4.58K</td>\n",
       "      <td>4.58K</td>\n",
       "      <td>51.07K</td>\n",
       "      <td>5.6K</td>\n",
       "      <td>102.14K</td>\n",
       "      <td>89.22K</td>\n",
       "      <td>541.32K</td>\n",
       "      <td>51.33K</td>\n",
       "      <td>90.9K</td>\n",
       "      <td>13.57K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Dataset               Cora IMDB-BINARY IMDB-MULTI   MUTAG     NCI1   NCI109 PROTEINS   PubMed REDDIT-BINARY US-county-demos-BachelorRate US-county-demos-BirthRate US-county-demos-DeathRate US-county-demos-Election US-county-demos-MedianIncome US-county-demos-MigraRate US-county-demos-UnemploymentRate     ZINC amazon_ratings citeseer minesweeper roman_empire tolokers\n",
       "Model                                                                                                                                                                                                                                                                                                                                                                           \n",
       "allsettransformer   60.26K     114.24K     111.3K  80.77K   57.47K  221.57K   14.34K  280.83K       106.18K                      316.93K                    30.21K                   316.93K                  217.34K                      217.34K                    80.64K                          105.86K  106.82K        155.91K  132.87K     118.02K      257.17K  217.99K\n",
       "edgnn              113.29K       9.86K     27.01K   5.73K   88.19K   88.32K     5.6K  147.59K         5.83K                        84.1K                     84.1K                     84.1K                   21.57K                       21.57K                    21.57K                           21.57K   22.53K        122.24K   258.5K       21.7K       41.49K   21.89K\n",
       "unignn2            109.06K     100.61K      8.32K   84.1K  104.32K    4.61K   21.31K  114.56K         68.1K                        3.55K                    13.25K                    51.07K                    4.58K                        4.58K                    51.07K                             5.6K  102.14K         89.22K  541.32K      51.33K        90.9K   13.57K"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
